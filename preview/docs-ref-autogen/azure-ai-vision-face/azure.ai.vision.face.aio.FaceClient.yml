### YamlMime:PythonClass
uid: azure.ai.vision.face.aio.FaceClient
name: FaceClient
fullName: azure.ai.vision.face.aio.FaceClient
module: azure.ai.vision.face.aio
inheritances:
- azure.ai.vision.face.aio._client.FaceClient
summary: FaceClient.
constructor:
  syntax: 'FaceClient(endpoint: str, credential: AzureKeyCredential | AsyncTokenCredential,
    **kwargs: Any)'
  parameters:
  - name: endpoint
    description: 'Supported Cognitive Services endpoints (protocol and hostname, for
      example:

      [https:/](https:/)/{resource-name}.cognitiveservices.azure.com). Required.'
    isRequired: true
    types:
    - <xref:str>
  - name: credential
    description: 'Credential used to authenticate requests to the service. Is either
      a

      AzureKeyCredential type or a TokenCredential type. Required.'
    isRequired: true
    types:
    - <xref:azure.core.credentials.AzureKeyCredential>
    - <xref:azure.core.credentials_async.AsyncTokenCredential>
  keywordOnlyParameters:
  - name: api_version
    description: 'API Version. Default value is "v1.1-preview.1". Note that overriding
      this

      default value may result in unsupported behavior.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.Versions>
methods:
- uid: azure.ai.vision.face.aio.FaceClient.close
  name: close
  signature: async close() -> None
- uid: azure.ai.vision.face.aio.FaceClient.detect
  name: detect
  summary: "Detect human faces in an image, return face rectangles, and optionally\
    \ with faceIds, landmarks,\nand attributes.\n\n   [!IMPORTANT]\n   To mitigate\
    \ potential misuse that can subject people to stereotyping, discrimination, or\n\
    \nunfair denial of services, we are retiring Face API attributes that predict\
    \ emotion, gender,\nage, smile, facial hair, hair, and makeup. Read more about\
    \ this decision\n[https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/](https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/).\n\
    \n* No image will be stored. Only the extracted face feature(s) will be stored\
    \ on server. The \n\nfaceId is an identifier of the face feature and will be used\
    \ in Face - Identify, Face - Verify,\nand Face - Find Similar. The stored face\
    \ features will expire and be deleted at the time\nspecified by faceIdTimeToLive\
    \ after the original detection call.\n* Optional parameters include faceId, landmarks,\
    \ and attributes. Attributes include headPose,\nglasses, occlusion, accessories,\
    \ blur, exposure, noise, mask, and qualityForRecognition. Some\nof the results\
    \ returned for specific attributes may not be highly accurate.\n* JPEG, PNG, GIF\
    \ (the first frame), and BMP format are supported. The allowed image file size\n\
    is from 1KB to 6MB.\n* The minimum detectable face size is 36x36 pixels in an\
    \ image no larger than 1920x1080 pixels.\nImages with dimensions higher than 1920x1080\
    \ pixels will need a proportionally larger minimum\nface size.\n* Up to 100 faces\
    \ can be returned for an image. Faces are ranked by face rectangle size from\n\
    large to small.\n* For optimal results when querying Face - Identify, Face - Verify,\
    \ and Face - Find Similar\n('returnFaceId' is true), please use faces that are:\
    \ frontal, clear, and with a minimum size of\n200x200 pixels (100 pixels between\
    \ eyes).\n* Different 'detectionModel' values can be provided. To use and compare\
    \ different detection\nmodels, please refer to\n[https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model)\n\
    \n   * 'detection_02': Face attributes and landmarks are disabled if you choose\
    \ this detection \n\nmodel.\n   * 'detection_03': Face attributes (mask and headPose\
    \ only) and landmarks are supported if you \n\nchoose this detection model.\n\n\
    * Different 'recognitionModel' values are provided. If follow-up operations like\
    \ Verify, \n\nIdentify, Find Similar are needed, please specify the recognition\
    \ model with 'recognitionModel'\nparameter. The default value for 'recognitionModel'\
    \ is 'recognition_01', if latest model\nneeded, please explicitly specify the\
    \ model you need in this parameter. Once specified, the\ndetected faceIds will\
    \ be associated with the specified recognition model. More details, please\nrefer\
    \ to\n[https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model)."
  signature: 'async detect(image_content: bytes, *, detection_model: str | FaceDetectionModel,
    recognition_model: str | FaceRecognitionModel, return_face_id: bool, return_face_attributes:
    List[str | FaceAttributeType] | None = None, return_face_landmarks: bool | None
    = None, return_recognition_model: bool | None = None, face_id_time_to_live: int
    | None = None, **kwargs: Any) -> List[FaceDetectionResult]'
  parameters:
  - name: image_content
    description: The input image binary. Required.
    isRequired: true
    types:
    - <xref:bytes>
  keywordOnlyParameters:
  - name: detection_model
    description: 'The ''detectionModel'' associated with the detected faceIds. Supported

      ''detectionModel'' values include ''detection_01'', ''detection_02'' and ''detection_03''.
      The default

      value is ''detection_01''. Known values are: "detection_01", "detection_02",
      and "detection_03".

      Required.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.FaceDetectionModel>
  - name: recognition_model
    description: 'The ''recognitionModel'' associated with the detected faceIds.

      Supported ''recognitionModel'' values include ''recognition_01'', ''recognition_02'',

      ''recognition_03'' or ''recognition_04''. The default value is ''recognition_01''.
      ''recognition_04''

      is recommended since its accuracy is improved on faces wearing masks compared
      with

      ''recognition_03'', and its overall accuracy is improved compared with ''recognition_01''
      and

      ''recognition_02''. Known values are: "recognition_01", "recognition_02", "recognition_03",
      and

      "recognition_04". Required.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.FaceRecognitionModel>
  - name: return_face_id
    description: Return faceIds of the detected faces or not. Required.
    types:
    - <xref:bool>
  - name: return_face_attributes
    description: 'Analyze and return the one or more specified face attributes

      in the comma-separated string like ''returnFaceAttributes=headPose,glasses''.
      Face attribute

      analysis has additional computational and time cost. Default value is None.'
    types:
    - <xref:list>[<xref:str>
    - <xref:azure.ai.vision.face.models.FaceAttributeType>]
  - name: return_face_landmarks
    description: 'Return face landmarks of the detected faces or not. The default

      value is false. Default value is None.'
    types:
    - <xref:bool>
  - name: return_recognition_model
    description: 'Return ''recognitionModel'' or not. The default value is

      false. Default value is None.'
    types:
    - <xref:bool>
  - name: face_id_time_to_live
    description: 'The number of seconds for the face ID being cached. Supported

      range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).
      Default value

      is None.'
    types:
    - <xref:int>
  return:
    description: list of FaceDetectionResult
    types:
    - <xref:list>[<xref:azure.ai.vision.face.models.FaceDetectionResult>]
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
  examples:
  - "<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"xml:space\": \"preserve\", \"force\": false, \"language\"\
    : \"python\", \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n\
    \   # response body for status code(s): 200\n   response == [\n       {\n    \
    \       \"faceRectangle\": {\n               \"height\": 0,  # The height of the\
    \ rectangle, in pixels.\n                 Required.\n               \"left\":\
    \ 0,  # The distance from the left edge if the image to\n                 the\
    \ left edge of the rectangle, in pixels. Required.\n               \"top\": 0,\
    \  # The distance from the top edge if the image to\n                 the top\
    \ edge of the rectangle, in pixels. Required.\n               \"width\": 0  #\
    \ The width of the rectangle, in pixels.\n                 Required.\n       \
    \    },\n           \"faceAttributes\": {\n               \"accessories\": [\n\
    \                   {\n                       \"confidence\": 0.0,  # Confidence\
    \ level of the\n                         accessory type. Range between [0,1].\
    \ Required.\n                       \"type\": \"str\"  # Type of the accessory.\n\
    \                         Required. Known values are: \"headwear\", \"glasses\"\
    , and \"mask\".\n                   }\n               ],\n               \"age\"\
    : 0.0,  # Optional. Age in years.\n               \"blur\": {\n              \
    \     \"blurLevel\": \"str\",  # An enum value indicating level\n            \
    \         of blurriness. Required. Known values are: \"low\", \"medium\", and\n\
    \                     \"high\".\n                   \"value\": 0.0  # A number\
    \ indicating level of\n                     blurriness ranging from 0 to 1. Required.\n\
    \               },\n               \"exposure\": {\n                   \"exposureLevel\"\
    : \"str\",  # An enum value indicating\n                     level of exposure.\
    \ Required. Known values are: \"underExposure\",\n                     \"goodExposure\"\
    , and \"overExposure\".\n                   \"value\": 0.0  # A number indicating\
    \ level of exposure\n                     level ranging from 0 to 1. [0, 0.25)\
    \ is under exposure. [0.25, 0.75)\n                     is good exposure. [0.75,\
    \ 1] is over exposure. Required.\n               },\n               \"facialHair\"\
    : {\n                   \"beard\": 0.0,  # A number ranging from 0 to 1\n    \
    \                 indicating a level of confidence associated with a property.\n\
    \                     Required.\n                   \"moustache\": 0.0,  # A number\
    \ ranging from 0 to 1\n                     indicating a level of confidence associated\
    \ with a property.\n                     Required.\n                   \"sideburns\"\
    : 0.0  # A number ranging from 0 to 1\n                     indicating a level\
    \ of confidence associated with a property.\n                     Required.\n\
    \               },\n               \"glasses\": \"str\",  # Optional. Glasses\
    \ type if any of the\n                 face. Known values are: \"noGlasses\",\
    \ \"readingGlasses\", \"sunglasses\", and\n                 \"swimmingGoggles\"\
    .\n               \"hair\": {\n                   \"bald\": 0.0,  # A number describing\
    \ confidence level\n                     of whether the person is bald. Required.\n\
    \                   \"hairColor\": [\n                       {\n             \
    \              \"color\": \"str\",  # Name of the hair\n                     \
    \        color. Required. Known values are: \"unknown\", \"white\",\n        \
    \                     \"gray\", \"blond\", \"brown\", \"red\", \"black\", and\
    \ \"other\".\n                           \"confidence\": 0.0  # Confidence level\n\
    \                             of the color. Range between [0,1]. Required.\n \
    \                      }\n                   ],\n                   \"invisible\"\
    : bool  # A boolean value describing\n                     whether the hair is\
    \ visible in the image. Required.\n               },\n               \"headPose\"\
    : {\n                   \"pitch\": 0.0,  # Value of angles. Required.\n      \
    \             \"roll\": 0.0,  # Value of angles. Required.\n                 \
    \  \"yaw\": 0.0  # Value of angles. Required.\n               },\n           \
    \    \"mask\": {\n                   \"noseAndMouthCovered\": bool,  # A boolean\
    \ value\n                     indicating whether nose and mouth are covered. Required.\n\
    \                   \"type\": \"str\"  # Type of the mask. Required. Known\n \
    \                    values are: \"faceMask\", \"noMask\", \"otherMaskOrOcclusion\"\
    , and\n                     \"uncertain\".\n               },\n              \
    \ \"noise\": {\n                   \"noiseLevel\": \"str\",  # An enum value indicating\n\
    \                     level of noise. Required. Known values are: \"low\", \"\
    medium\", and\n                     \"high\".\n                   \"value\": 0.0\
    \  # A number indicating level of noise\n                     level ranging from\
    \ 0 to 1. [0, 0.25) is under exposure. [0.25, 0.75)\n                     is good\
    \ exposure. [0.75, 1] is over exposure. [0, 0.3) is low noise\n              \
    \       level. [0.3, 0.7) is medium noise level. [0.7, 1] is high noise\n    \
    \                 level. Required.\n               },\n               \"occlusion\"\
    : {\n                   \"eyeOccluded\": bool,  # A boolean value indicating\n\
    \                     whether eyes are occluded. Required.\n                 \
    \  \"foreheadOccluded\": bool,  # A boolean value\n                     indicating\
    \ whether forehead is occluded. Required.\n                   \"mouthOccluded\"\
    : bool  # A boolean value indicating\n                     whether the mouth is\
    \ occluded. Required.\n               },\n               \"qualityForRecognition\"\
    : \"str\",  # Optional. Properties\n                 describing the overall image\
    \ quality regarding whether the image being\n                 used in the detection\
    \ is of sufficient quality to attempt face\n                 recognition on. Known\
    \ values are: \"low\", \"medium\", and \"high\".\n               \"smile\": 0.0\
    \  # Optional. Smile intensity, a number between\n                 [0,1].\n  \
    \         },\n           \"faceId\": \"str\",  # Optional. Unique faceId of the\
    \ detected face,\n             created by detection API and it will expire 24\
    \ hours after the detection\n             call. To return this, it requires 'returnFaceId'\
    \ parameter to be true.\n           \"faceLandmarks\": {\n               \"eyeLeftBottom\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"eyeLeftInner\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"eyeLeftOuter\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"eyeLeftTop\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"eyeRightBottom\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"eyeRightInner\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"eyeRightOuter\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"eyeRightTop\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"eyebrowLeftInner\": {\n                   \"x\"\
    : 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"eyebrowLeftOuter\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"eyebrowRightInner\": {\n                   \"x\": 0.0,  # The\
    \ horizontal component, in pixels.\n                     Required.\n         \
    \          \"y\": 0.0  # The vertical component, in pixels.\n                \
    \     Required.\n               },\n               \"eyebrowRightOuter\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"mouthLeft\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"mouthRight\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"noseLeftAlarOutTip\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"noseLeftAlarTop\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"noseRightAlarOutTip\": {\n              \
    \     \"x\": 0.0,  # The horizontal component, in pixels.\n                  \
    \   Required.\n                   \"y\": 0.0  # The vertical component, in pixels.\n\
    \                     Required.\n               },\n               \"noseRightAlarTop\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"noseRootLeft\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"noseRootRight\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"noseTip\": {\n \
    \                  \"x\": 0.0,  # The horizontal component, in pixels.\n     \
    \                Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"pupilLeft\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"pupilRight\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"underLipBottom\":\
    \ {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"underLipTop\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"upperLipBottom\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"upperLipTop\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               }\n           },\n\
    \           \"recognitionModel\": \"str\"  # Optional. The 'recognitionModel'\n\
    \             associated with this faceId. This is only returned when\n      \
    \       'returnRecognitionModel' is explicitly set as true. Known values are:\n\
    \             \"recognition_01\", \"recognition_02\", \"recognition_03\", and\
    \ \"recognition_04\".\n       }\n   ]\n   ````\n"
- uid: azure.ai.vision.face.aio.FaceClient.detect_from_url
  name: detect_from_url
  summary: "Detect human faces in an image, return face rectangles, and optionally\
    \ with faceIds, landmarks,\nand attributes.\n\n   [!IMPORTANT]\n   To mitigate\
    \ potential misuse that can subject people to stereotyping, discrimination, or\n\
    \nunfair denial of services, we are retiring Face API attributes that predict\
    \ emotion, gender,\nage, smile, facial hair, hair, and makeup. Read more about\
    \ this decision\n[https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/](https://azure.microsoft.com/blog/responsible-ai-investments-and-safeguards-for-facial-recognition/).\n\
    \n* No image will be stored. Only the extracted face feature(s) will be stored\
    \ on server. The \n\nfaceId is an identifier of the face feature and will be used\
    \ in Face - Identify, Face - Verify,\nand Face - Find Similar. The stored face\
    \ features will expire and be deleted at the time\nspecified by faceIdTimeToLive\
    \ after the original detection call.\n* Optional parameters include faceId, landmarks,\
    \ and attributes. Attributes include headPose,\nglasses, occlusion, accessories,\
    \ blur, exposure, noise, mask, and qualityForRecognition. Some\nof the results\
    \ returned for specific attributes may not be highly accurate.\n* JPEG, PNG, GIF\
    \ (the first frame), and BMP format are supported. The allowed image file size\n\
    is from 1KB to 6MB.\n* The minimum detectable face size is 36x36 pixels in an\
    \ image no larger than 1920x1080 pixels.\nImages with dimensions higher than 1920x1080\
    \ pixels will need a proportionally larger minimum\nface size.\n* Up to 100 faces\
    \ can be returned for an image. Faces are ranked by face rectangle size from\n\
    large to small.\n* For optimal results when querying Face - Identify, Face - Verify,\
    \ and Face - Find Similar\n('returnFaceId' is true), please use faces that are:\
    \ frontal, clear, and with a minimum size of\n200x200 pixels (100 pixels between\
    \ eyes).\n* Different 'detectionModel' values can be provided. To use and compare\
    \ different detection\nmodels, please refer to\n[https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-detection-model)\n\
    \n   * 'detection_02': Face attributes and landmarks are disabled if you choose\
    \ this detection \n\nmodel.\n   * 'detection_03': Face attributes (mask and headPose\
    \ only) and landmarks are supported if you \n\nchoose this detection model.\n\n\
    * Different 'recognitionModel' values are provided. If follow-up operations like\
    \ Verify, \n\nIdentify, Find Similar are needed, please specify the recognition\
    \ model with 'recognitionModel'\nparameter. The default value for 'recognitionModel'\
    \ is 'recognition_01', if latest model\nneeded, please explicitly specify the\
    \ model you need in this parameter. Once specified, the\ndetected faceIds will\
    \ be associated with the specified recognition model. More details, please\nrefer\
    \ to\n[https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/specify-recognition-model)."
  signature: 'async detect_from_url(*, url: str, content_type: str = ''application/json'',
    detection_model: str | _models.FaceDetectionModel, recognition_model: str | _models.FaceRecognitionModel,
    return_face_id: bool, return_face_attributes: List[str | _models.FaceAttributeType]
    | None = None, return_face_landmarks: bool | None = None, return_recognition_model:
    bool | None = None, face_id_time_to_live: int | None = None, **kwargs: Any) ->
    List[_models.FaceDetectionResult]'
  parameters:
  - name: body
    description: Is either a JSON type or a IO[bytes] type. Required.
    isRequired: true
    types:
    - <xref:JSON>
    - <xref:typing.IO>[<xref:bytes>]
  keywordOnlyParameters:
  - name: url
    description: URL of input image. Required when body is not set.
    types:
    - <xref:str>
  - name: detection_model
    description: 'The ''detectionModel'' associated with the detected faceIds. Supported

      ''detectionModel'' values include ''detection_01'', ''detection_02'' and ''detection_03''.
      The default

      value is ''detection_01''. Known values are: "detection_01", "detection_02",
      and "detection_03".

      Required.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.FaceDetectionModel>
  - name: recognition_model
    description: 'The ''recognitionModel'' associated with the detected faceIds.

      Supported ''recognitionModel'' values include ''recognition_01'', ''recognition_02'',

      ''recognition_03'' or ''recognition_04''. The default value is ''recognition_01''.
      ''recognition_04''

      is recommended since its accuracy is improved on faces wearing masks compared
      with

      ''recognition_03'', and its overall accuracy is improved compared with ''recognition_01''
      and

      ''recognition_02''. Known values are: "recognition_01", "recognition_02", "recognition_03",
      and

      "recognition_04". Required.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.FaceRecognitionModel>
  - name: return_face_id
    description: Return faceIds of the detected faces or not. Required.
    types:
    - <xref:bool>
  - name: return_face_attributes
    description: 'Analyze and return the one or more specified face attributes

      in the comma-separated string like ''returnFaceAttributes=headPose,glasses''.
      Face attribute

      analysis has additional computational and time cost. Default value is None.'
    types:
    - <xref:list>[<xref:str>
    - <xref:azure.ai.vision.face.models.FaceAttributeType>]
  - name: return_face_landmarks
    description: 'Return face landmarks of the detected faces or not. The default

      value is false. Default value is None.'
    types:
    - <xref:bool>
  - name: return_recognition_model
    description: 'Return ''recognitionModel'' or not. The default value is

      false. Default value is None.'
    types:
    - <xref:bool>
  - name: face_id_time_to_live
    description: 'The number of seconds for the face ID being cached. Supported

      range from 60 seconds up to 86400 seconds. The default value is 86400 (24 hours).
      Default value

      is None.'
    types:
    - <xref:int>
  return:
    description: list of FaceDetectionResult
    types:
    - <xref:list>[<xref:azure.ai.vision.face.models.FaceDetectionResult>]
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
  examples:
  - "<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"xml:space\": \"preserve\", \"force\": false, \"language\"\
    : \"python\", \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n\
    \   # JSON input template you can fill out and use as your body input.\n   body\
    \ = {\n       \"url\": \"str\"  # URL of input image. Required.\n   }\n\n   #\
    \ response body for status code(s): 200\n   response == [\n       {\n        \
    \   \"faceRectangle\": {\n               \"height\": 0,  # The height of the rectangle,\
    \ in pixels.\n                 Required.\n               \"left\": 0,  # The distance\
    \ from the left edge if the image to\n                 the left edge of the rectangle,\
    \ in pixels. Required.\n               \"top\": 0,  # The distance from the top\
    \ edge if the image to\n                 the top edge of the rectangle, in pixels.\
    \ Required.\n               \"width\": 0  # The width of the rectangle, in pixels.\n\
    \                 Required.\n           },\n           \"faceAttributes\": {\n\
    \               \"accessories\": [\n                   {\n                   \
    \    \"confidence\": 0.0,  # Confidence level of the\n                       \
    \  accessory type. Range between [0,1]. Required.\n                       \"type\"\
    : \"str\"  # Type of the accessory.\n                         Required. Known\
    \ values are: \"headwear\", \"glasses\", and \"mask\".\n                   }\n\
    \               ],\n               \"age\": 0.0,  # Optional. Age in years.\n\
    \               \"blur\": {\n                   \"blurLevel\": \"str\",  # An\
    \ enum value indicating level\n                     of blurriness. Required. Known\
    \ values are: \"low\", \"medium\", and\n                     \"high\".\n     \
    \              \"value\": 0.0  # A number indicating level of\n              \
    \       blurriness ranging from 0 to 1. Required.\n               },\n       \
    \        \"exposure\": {\n                   \"exposureLevel\": \"str\",  # An\
    \ enum value indicating\n                     level of exposure. Required. Known\
    \ values are: \"underExposure\",\n                     \"goodExposure\", and \"\
    overExposure\".\n                   \"value\": 0.0  # A number indicating level\
    \ of exposure\n                     level ranging from 0 to 1. [0, 0.25) is under\
    \ exposure. [0.25, 0.75)\n                     is good exposure. [0.75, 1] is\
    \ over exposure. Required.\n               },\n               \"facialHair\":\
    \ {\n                   \"beard\": 0.0,  # A number ranging from 0 to 1\n    \
    \                 indicating a level of confidence associated with a property.\n\
    \                     Required.\n                   \"moustache\": 0.0,  # A number\
    \ ranging from 0 to 1\n                     indicating a level of confidence associated\
    \ with a property.\n                     Required.\n                   \"sideburns\"\
    : 0.0  # A number ranging from 0 to 1\n                     indicating a level\
    \ of confidence associated with a property.\n                     Required.\n\
    \               },\n               \"glasses\": \"str\",  # Optional. Glasses\
    \ type if any of the\n                 face. Known values are: \"noGlasses\",\
    \ \"readingGlasses\", \"sunglasses\", and\n                 \"swimmingGoggles\"\
    .\n               \"hair\": {\n                   \"bald\": 0.0,  # A number describing\
    \ confidence level\n                     of whether the person is bald. Required.\n\
    \                   \"hairColor\": [\n                       {\n             \
    \              \"color\": \"str\",  # Name of the hair\n                     \
    \        color. Required. Known values are: \"unknown\", \"white\",\n        \
    \                     \"gray\", \"blond\", \"brown\", \"red\", \"black\", and\
    \ \"other\".\n                           \"confidence\": 0.0  # Confidence level\n\
    \                             of the color. Range between [0,1]. Required.\n \
    \                      }\n                   ],\n                   \"invisible\"\
    : bool  # A boolean value describing\n                     whether the hair is\
    \ visible in the image. Required.\n               },\n               \"headPose\"\
    : {\n                   \"pitch\": 0.0,  # Value of angles. Required.\n      \
    \             \"roll\": 0.0,  # Value of angles. Required.\n                 \
    \  \"yaw\": 0.0  # Value of angles. Required.\n               },\n           \
    \    \"mask\": {\n                   \"noseAndMouthCovered\": bool,  # A boolean\
    \ value\n                     indicating whether nose and mouth are covered. Required.\n\
    \                   \"type\": \"str\"  # Type of the mask. Required. Known\n \
    \                    values are: \"faceMask\", \"noMask\", \"otherMaskOrOcclusion\"\
    , and\n                     \"uncertain\".\n               },\n              \
    \ \"noise\": {\n                   \"noiseLevel\": \"str\",  # An enum value indicating\n\
    \                     level of noise. Required. Known values are: \"low\", \"\
    medium\", and\n                     \"high\".\n                   \"value\": 0.0\
    \  # A number indicating level of noise\n                     level ranging from\
    \ 0 to 1. [0, 0.25) is under exposure. [0.25, 0.75)\n                     is good\
    \ exposure. [0.75, 1] is over exposure. [0, 0.3) is low noise\n              \
    \       level. [0.3, 0.7) is medium noise level. [0.7, 1] is high noise\n    \
    \                 level. Required.\n               },\n               \"occlusion\"\
    : {\n                   \"eyeOccluded\": bool,  # A boolean value indicating\n\
    \                     whether eyes are occluded. Required.\n                 \
    \  \"foreheadOccluded\": bool,  # A boolean value\n                     indicating\
    \ whether forehead is occluded. Required.\n                   \"mouthOccluded\"\
    : bool  # A boolean value indicating\n                     whether the mouth is\
    \ occluded. Required.\n               },\n               \"qualityForRecognition\"\
    : \"str\",  # Optional. Properties\n                 describing the overall image\
    \ quality regarding whether the image being\n                 used in the detection\
    \ is of sufficient quality to attempt face\n                 recognition on. Known\
    \ values are: \"low\", \"medium\", and \"high\".\n               \"smile\": 0.0\
    \  # Optional. Smile intensity, a number between\n                 [0,1].\n  \
    \         },\n           \"faceId\": \"str\",  # Optional. Unique faceId of the\
    \ detected face,\n             created by detection API and it will expire 24\
    \ hours after the detection\n             call. To return this, it requires 'returnFaceId'\
    \ parameter to be true.\n           \"faceLandmarks\": {\n               \"eyeLeftBottom\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"eyeLeftInner\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"eyeLeftOuter\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"eyeLeftTop\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"eyeRightBottom\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"eyeRightInner\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"eyeRightOuter\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"eyeRightTop\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"eyebrowLeftInner\": {\n                   \"x\"\
    : 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"eyebrowLeftOuter\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"eyebrowRightInner\": {\n                   \"x\": 0.0,  # The\
    \ horizontal component, in pixels.\n                     Required.\n         \
    \          \"y\": 0.0  # The vertical component, in pixels.\n                \
    \     Required.\n               },\n               \"eyebrowRightOuter\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"mouthLeft\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"mouthRight\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"noseLeftAlarOutTip\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"noseLeftAlarTop\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"noseRightAlarOutTip\": {\n              \
    \     \"x\": 0.0,  # The horizontal component, in pixels.\n                  \
    \   Required.\n                   \"y\": 0.0  # The vertical component, in pixels.\n\
    \                     Required.\n               },\n               \"noseRightAlarTop\"\
    : {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"noseRootLeft\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"noseRootRight\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"noseTip\": {\n \
    \                  \"x\": 0.0,  # The horizontal component, in pixels.\n     \
    \                Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               },\n            \
    \   \"pupilLeft\": {\n                   \"x\": 0.0,  # The horizontal component,\
    \ in pixels.\n                     Required.\n                   \"y\": 0.0  #\
    \ The vertical component, in pixels.\n                     Required.\n       \
    \        },\n               \"pupilRight\": {\n                   \"x\": 0.0,\
    \  # The horizontal component, in pixels.\n                     Required.\n  \
    \                 \"y\": 0.0  # The vertical component, in pixels.\n         \
    \            Required.\n               },\n               \"underLipBottom\":\
    \ {\n                   \"x\": 0.0,  # The horizontal component, in pixels.\n\
    \                     Required.\n                   \"y\": 0.0  # The vertical\
    \ component, in pixels.\n                     Required.\n               },\n \
    \              \"underLipTop\": {\n                   \"x\": 0.0,  # The horizontal\
    \ component, in pixels.\n                     Required.\n                   \"\
    y\": 0.0  # The vertical component, in pixels.\n                     Required.\n\
    \               },\n               \"upperLipBottom\": {\n                   \"\
    x\": 0.0,  # The horizontal component, in pixels.\n                     Required.\n\
    \                   \"y\": 0.0  # The vertical component, in pixels.\n       \
    \              Required.\n               },\n               \"upperLipTop\": {\n\
    \                   \"x\": 0.0,  # The horizontal component, in pixels.\n    \
    \                 Required.\n                   \"y\": 0.0  # The vertical component,\
    \ in pixels.\n                     Required.\n               }\n           },\n\
    \           \"recognitionModel\": \"str\"  # Optional. The 'recognitionModel'\n\
    \             associated with this faceId. This is only returned when\n      \
    \       'returnRecognitionModel' is explicitly set as true. Known values are:\n\
    \             \"recognition_01\", \"recognition_02\", \"recognition_03\", and\
    \ \"recognition_04\".\n       }\n   ]\n   ````\n"
- uid: azure.ai.vision.face.aio.FaceClient.find_similar
  name: find_similar
  summary: 'Given query face''s faceId, to search the similar-looking faces from a
    faceId array. A faceId

    array contains the faces created by Detect.


    Depending on the input the returned similar faces list contains faceIds or persistedFaceIds

    ranked by similarity.


    Find similar has two working modes, "matchPerson" and "matchFace". "matchPerson"
    is the default

    mode that it tries to find faces of the same person as possible by using internal
    same-person

    thresholds. It is useful to find a known person''s other photos. Note that an
    empty list will be

    returned if no faces pass the internal thresholds. "matchFace" mode ignores same-person

    thresholds and returns ranked similar faces anyway, even the similarity is low.
    It can be used

    in the cases like searching celebrity-looking faces.


    The ''recognitionModel'' associated with the query faceId should be the same as
    the

    ''recognitionModel'' used by the target faceId array.'
  signature: 'async find_similar(body: ~collections.abc.MutableMapping[str, ~typing.Any]
    | ~typing.IO[bytes] = <object object>, *, face_id: str = <object object>, face_ids:
    ~typing.List[str] = <object object>, max_num_of_candidates_returned: int | None
    = None, mode: str | ~azure.ai.vision.face.models._enums.FindSimilarMatchMode |
    None = None, **kwargs: ~typing.Any) -> List[FaceFindSimilarResult]'
  parameters:
  - name: body
    description: Is either a JSON type or a IO[bytes] type. Required.
    isRequired: true
    types:
    - <xref:JSON>
    - <xref:typing.IO>[<xref:bytes>]
  keywordOnlyParameters:
  - name: face_id
    description: 'faceId of the query face. User needs to call "Detect" first to get
      a valid

      faceId. Note that this faceId is not persisted and will expire 24 hours after
      the detection

      call. Required.'
    types:
    - <xref:str>
  - name: face_ids
    description: 'An array of candidate faceIds. All of them are created by "Detect"
      and the

      faceIds will expire 24 hours after the detection call. The number of faceIds
      is limited to

      1000. Required.'
    types:
    - <xref:list>[<xref:str>]
  - name: max_num_of_candidates_returned
    description: 'The number of top similar faces returned. The valid

      range is [1, 1000]. Default value is 20. Default value is None.'
    types:
    - <xref:int>
  - name: mode
    description: 'Similar face searching mode. It can be ''matchPerson'' or ''matchFace''.
      Default

      value is ''matchPerson''. Known values are: "matchPerson" and "matchFace". Default
      value is None.'
    types:
    - <xref:str>
    - <xref:azure.ai.vision.face.models.FindSimilarMatchMode>
  return:
    description: list of FaceFindSimilarResult
    types:
    - <xref:list>[<xref:azure.ai.vision.face.models.FaceFindSimilarResult>]
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
  examples:
  - "<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"xml:space\": \"preserve\", \"force\": false, \"language\"\
    : \"python\", \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n\
    \   # JSON input template you can fill out and use as your body input.\n   body\
    \ = {\n       \"faceId\": \"str\",  # faceId of the query face. User needs to\
    \ call \"Detect\"\n         first to get a valid faceId. Note that this faceId\
    \ is not persisted and will\n         expire 24 hours after the detection call.\
    \ Required.\n       \"faceIds\": [\n           \"str\"  # An array of candidate\
    \ faceIds. All of them are created by\n             \"Detect\" and the faceIds\
    \ will expire 24 hours after the detection call. The\n             number of faceIds\
    \ is limited to 1000. Required.\n       ],\n       \"maxNumOfCandidatesReturned\"\
    : 0,  # Optional. The number of top similar faces\n         returned. The valid\
    \ range is [1, 1000]. Default value is 20.\n       \"mode\": \"str\"  # Optional.\
    \ Similar face searching mode. It can be\n         'matchPerson' or 'matchFace'.\
    \ Default value is 'matchPerson'. Known values are:\n         \"matchPerson\"\
    \ and \"matchFace\".\n   }\n\n   # response body for status code(s): 200\n   response\
    \ == [\n       {\n           \"confidence\": 0.0,  # Confidence value of the candidate.\
    \ The higher\n             confidence, the more similar. Range between [0,1].\
    \ Required.\n           \"faceId\": \"str\",  # Optional. faceId of candidate\
    \ face when find by\n             faceIds. faceId is created by \"Detect\" and\
    \ will expire 24 hours after the\n             detection call.\n           \"\
    persistedFaceId\": \"str\"  # Optional. persistedFaceId of candidate\n       \
    \      face when find by faceListId or largeFaceListId. persistedFaceId in face\n\
    \             list/large face list is persisted and will not expire.\n       }\n\
    \   ]\n   ````\n"
- uid: azure.ai.vision.face.aio.FaceClient.group
  name: group
  summary: "Divide candidate faces into groups based on face similarity.\n\n# >\n\n\
    * The output is one or more disjointed face groups and a messyGroup. A face group\
    \ contains \n\nfaces that have similar looking, often of the same person. Face\
    \ groups are ranked by group\nsize, i.e. number of faces. Notice that faces belonging\
    \ to a same person might be split into\nseveral groups in the result.\n* MessyGroup\
    \ is a special face group containing faces that cannot find any similar counterpart\n\
    face from original faces. The messyGroup will not appear in the result if all\
    \ faces found their\ncounterparts.\n* Group API needs at least 2 candidate faces\
    \ and 1000 at most. We suggest to try \"Verify Face\nTo Face\" when you only have\
    \ 2 candidate faces.\n* The 'recognitionModel' associated with the query faces'\
    \ faceIds should be the same.\n\nparam body:\n   Is either a JSON type or a IO[bytes]\
    \ type. Required.\n\ntype body:\n   JSON or IO[bytes]\n\nkeyword face_ids:\n \
    \  Array of candidate faceIds created by \"Detect\". The maximum is 1000 faces.\n\
    \   Required.\n\nparamtype face_ids:\n   list[str]\n\nreturn:\n   FaceGroupingResult.\
    \ The FaceGroupingResult is compatible with MutableMapping\n\nrtype:\n   ~azure.ai.vision.face.models.FaceGroupingResult\n\
    \nraises ~azure.core.exceptions.HttpResponseError:\n\n\n## Example\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\"\
    , \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n   # JSON input\
    \ template you can fill out and use as your body input.\n   body = {\n       \"\
    faceIds\": [\n           \"str\"  # Array of candidate faceIds created by \"Detect\"\
    . The maximum\n             is 1000 faces. Required.\n       ]\n   }\n\n   # response\
    \ body for status code(s): 200\n   response == {\n       \"groups\": [\n     \
    \      [\n               \"str\"  # A partition of the original faces based on\
    \ face\n                 similarity. Groups are ranked by number of faces. Required.\n\
    \           ]\n       ],\n       \"messyGroup\": [\n           \"str\"  # Face\
    \ ids array of faces that cannot find any similar faces\n             from original\
    \ faces. Required.\n       ]\n   }\n   ````"
  signature: 'async group(body: ~collections.abc.MutableMapping[str, ~typing.Any]
    | ~typing.IO[bytes] = <object object>, *, face_ids: ~typing.List[str] = <object
    object>, **kwargs: ~typing.Any) -> FaceGroupingResult'
- uid: azure.ai.vision.face.aio.FaceClient.send_request
  name: send_request
  summary: 'Runs the network request through the client''s chained policies.


    ```


    >>> from azure.core.rest import HttpRequest

    >>> request = HttpRequest("GET", "https://www.example.org/")

    <HttpRequest [GET], url: ''https://www.example.org/''>

    >>> response = await client.send_request(request)

    <AsyncHttpResponse: 200 OK>

    ```


    For more information on this code flow, see [https://aka.ms/azsdk/dpcodegen/python/send_request](https://aka.ms/azsdk/dpcodegen/python/send_request)'
  signature: 'send_request(request: HttpRequest, *, stream: bool = False, **kwargs:
    Any) -> Awaitable[AsyncHttpResponse]'
  parameters:
  - name: request
    description: The network request you want to make. Required.
    isRequired: true
    types:
    - <xref:azure.core.rest.HttpRequest>
  keywordOnlyParameters:
  - name: stream
    description: Whether the response payload will be streamed. Defaults to False.
    types:
    - <xref:bool>
  return:
    description: The response of your network call. Does not do error handling on
      your response.
    types:
    - <xref:azure.core.rest.AsyncHttpResponse>
- uid: azure.ai.vision.face.aio.FaceClient.verify_face_to_face
  name: verify_face_to_face
  summary: "Verify whether two faces belong to a same person.\n\n   [!NOTE]\n\n  \
    \ * Higher face image quality means better identification precision. Please consider\
    \ \n\nhigh-quality faces: frontal, clear, and face size is 200x200 pixels (100\
    \ pixels between eyes)\nor bigger.\n\n   * For the scenarios that are sensitive\
    \ to accuracy please make your own judgment. \n\n   * The 'recognitionModel' associated\
    \ with the both faces should be the same."
  signature: 'async verify_face_to_face(body: ~collections.abc.MutableMapping[str,
    ~typing.Any] | ~typing.IO[bytes] = <object object>, *, face_id1: str = <object
    object>, face_id2: str = <object object>, **kwargs: ~typing.Any) -> FaceVerificationResult'
  parameters:
  - name: body
    description: Is either a JSON type or a IO[bytes] type. Required.
    isRequired: true
    types:
    - <xref:JSON>
    - <xref:typing.IO>[<xref:bytes>]
  keywordOnlyParameters:
  - name: face_id1
    description: The faceId of one face, come from "Detect". Required.
    types:
    - <xref:str>
  - name: face_id2
    description: The faceId of another face, come from "Detect". Required.
    types:
    - <xref:str>
  return:
    description: FaceVerificationResult. The FaceVerificationResult is compatible
      with MutableMapping
    types:
    - <xref:azure.ai.vision.face.models.FaceVerificationResult>
  exceptions:
  - type: azure.core.exceptions.HttpResponseError
  examples:
  - "<!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"xml:space\": \"preserve\", \"force\": false, \"language\"\
    : \"python\", \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n\
    \   # JSON input template you can fill out and use as your body input.\n   body\
    \ = {\n       \"faceId1\": \"str\",  # The faceId of one face, come from \"Detect\"\
    . Required.\n       \"faceId2\": \"str\"  # The faceId of another face, come from\
    \ \"Detect\". Required.\n   }\n\n   # response body for status code(s): 200\n\
    \   response == {\n       \"confidence\": 0.0,  # A number indicates the similarity\
    \ confidence of whether\n         two faces belong to the same person, or whether\
    \ the face belongs to the person.\n         By default, isIdentical is set to\
    \ True if similarity confidence is greater than\n         or equal to 0.5. This\
    \ is useful for advanced users to override 'isIdentical' and\n         fine-tune\
    \ the result on their own data. Required.\n       \"isIdentical\": bool  # True\
    \ if the two faces belong to the same person or the\n         face belongs to\
    \ the person, otherwise false. Required.\n   }\n   ````\n"
