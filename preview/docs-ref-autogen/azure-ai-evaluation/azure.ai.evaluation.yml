### YamlMime:PythonPackage
uid: azure.ai.evaluation
name: evaluation
fullName: azure.ai.evaluation
type: rootImport
functions:
- uid: azure.ai.evaluation.evaluate
  name: evaluate
  summary: "Evaluates target or data with built-in or custom evaluators. If both target\
    \ and data are provided,\n   data will be run through target function and then\
    \ results will be evaluated.\n\nEvaluate API can be used as follows:\n\n<!-- literal_block\
    \ {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\"\
    : [], \"xml:space\": \"preserve\", \"force\": false, \"language\": \"python\"\
    , \"highlight_args\": {}, \"linenos\": false} -->\n\n````python\n\n   from azure.ai.evaluation\
    \ import evaluate, RelevanceEvaluator, CoherenceEvaluator\n\n\n   model_config\
    \ = {\n       \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n\
    \       \"api_key\": os.environ.get(\"AZURE_OPENAI_KEY\"),\n       \"azure_deployment\"\
    : os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\")\n   }\n\n   coherence_eval = CoherenceEvaluator(model_config=model_config)\n\
    \   relevance_eval = RelevanceEvaluator(model_config=model_config)\n\n   path\
    \ = \"evaluate_test_data.jsonl\"\n   result = evaluate(\n       data=path,\n \
    \      evaluators={\n           \"coherence\": coherence_eval,\n           \"\
    relevance\": relevance_eval,\n       },\n       evaluator_config={\n         \
    \  \"coherence\": {\n               \"response\": \"${data.response}\",\n    \
    \           \"query\": \"${data.query}\"\n           },\n           \"relevance\"\
    : {\n               \"response\": \"${data.response}\",\n               \"context\"\
    : \"${data.context}\",\n               \"query\": \"${data.query}\"\n        \
    \   }\n       }\n   )\n   ````"
  signature: 'evaluate(*, evaluation_name: str | None = None, target: Callable | None
    = None, data: str | None = None, evaluators: Dict[str, Callable] | None = None,
    evaluator_config: Dict[str, Dict[str, str]] | None = None, azure_ai_project: AzureAIProject
    | None = None, output_path: str | None = None, **kwargs)'
  keywordOnlyParameters:
  - name: evaluation_name
    description: Display name of the evaluation.
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: target
    description: Target to be evaluated. *target* and *data* both cannot be None
    types:
    - <xref:typing.Optional>[<xref:typing.Callable>]
  - name: data
    description: 'Path to the data to be evaluated or passed to target if target is
      set.

      Only .jsonl format files are supported.  *target* and *data* both cannot be
      None'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: evaluators
    description: 'Evaluators to be used for evaluation. It should be a dictionary
      with key as alias for evaluator

      and value as the evaluator function.'
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:typing.Callable>]
  - name: evaluator_config
    description: 'Configuration for evaluators. The configuration should be a dictionary
      with evaluator

      names as keys and a dictionary of column mappings as values. The column mappings
      should be a dictionary with

      keys as the column names in the evaluator input and values as the column names
      in the input data or data

      generated by target.'
    types:
    - <xref:typing.Optional>[<xref:typing.Dict>[<xref:str>, <xref:typing.Dict>[<xref:str>,
      <xref:str>]]
  - name: output_path
    description: 'The local folder or file path to save evaluation results to if set.
      If folder path is provided

      the results will be saved to a file named *evaluation_results.json* in the folder.'
    types:
    - <xref:typing.Optional>[<xref:str>]
  - name: azure_ai_project
    description: Logs evaluation results to AI Studio if set.
    types:
    - <xref:typing.Optional>[<xref:azure.ai.evaluation.AzureAIProject>]
  return:
    description: Evaluation results.
    types:
    - <xref:dict>
classes:
- azure.ai.evaluation.AzureAIProject
- azure.ai.evaluation.AzureOpenAIModelConfiguration
- azure.ai.evaluation.BleuScoreEvaluator
- azure.ai.evaluation.ChatEvaluator
- azure.ai.evaluation.CoherenceEvaluator
- azure.ai.evaluation.ContentSafetyChatEvaluator
- azure.ai.evaluation.ContentSafetyEvaluator
- azure.ai.evaluation.F1ScoreEvaluator
- azure.ai.evaluation.FluencyEvaluator
- azure.ai.evaluation.GleuScoreEvaluator
- azure.ai.evaluation.GroundednessEvaluator
- azure.ai.evaluation.HateUnfairnessEvaluator
- azure.ai.evaluation.IndirectAttackEvaluator
- azure.ai.evaluation.MeteorScoreEvaluator
- azure.ai.evaluation.OpenAIModelConfiguration
- azure.ai.evaluation.ProtectedMaterialEvaluator
- azure.ai.evaluation.QAEvaluator
- azure.ai.evaluation.RelevanceEvaluator
- azure.ai.evaluation.RougeScoreEvaluator
- azure.ai.evaluation.SelfHarmEvaluator
- azure.ai.evaluation.SexualEvaluator
- azure.ai.evaluation.SimilarityEvaluator
- azure.ai.evaluation.ViolenceEvaluator
packages:
- azure.ai.evaluation.simulator
enums:
- azure.ai.evaluation.RougeType
