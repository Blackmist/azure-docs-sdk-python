### YamlMime:PythonClass
uid: azure.ai.evaluation.ContentSafetyChatEvaluator
name: ContentSafetyChatEvaluator
fullName: azure.ai.evaluation.ContentSafetyChatEvaluator
module: azure.ai.evaluation
inheritances:
- builtins.object
summary: "Initialize a content safety chat evaluator configured to evaluate content\
  \ safetry metrics for chat scenario.\n\n**Usage**\n\n<!-- literal_block {\"ids\"\
  : [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"xml:space\"\
  : \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\":\
  \ {}, \"linenos\": false} -->\n\n````python\n\n   azure_ai_project = {\n       \"\
  subscription_id\": \"<subscription_id>\",\n       \"resource_group_name\": \"<resource_group_name>\"\
  ,\n       \"project_name\": \"<project_name>\",\n   }\n   eval_fn = ContentSafetyChatEvaluator(azure_ai_project)\n\
  \   result = eval_fn(conversation=[\n       {\"role\": \"user\", \"content\": \"\
  What is the value of 2 + 2?\"},\n       {\"role\": \"assistant\", \"content\": \"\
  2 + 2 = 4\"}\n   ])\n   ````\n\n**Output format**\n\n<!-- literal_block {\"ids\"\
  : [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"xml:space\"\
  : \"preserve\", \"force\": false, \"language\": \"python\", \"highlight_args\":\
  \ {}, \"linenos\": false} -->\n\n````python\n\n   {\n       \"evaluation_per_turn\"\
  : {\n           \"violence\": [\"High\", \"Low\"],\n           \"violence_score\"\
  : [7.0, 3.0],\n           \"violence_reason\": \"Some reason\",\n           \"sexual\"\
  : [\"High\", \"Low\"],\n           \"sexual_score\": [7.0, 3.0],\n           \"\
  sexual_reason\": \"Some reason\",\n           \"self_harm\": [\"High\", \"Low\"\
  ],\n           \"self_harm_score\": [7.0, 3.0],\n           \"self_harm_reason\"\
  : \"Some reason\",\n           \"hate_unfairness\": [\"High\", \"Low\"],\n     \
  \      \"hate_unfairness_score\": [7.0, 3.0],\n           \"hate_unfairness_reason\"\
  : \"Some reason\"\n       },\n       \"violence\": \"Medium\",\n       \"violence_score\"\
  : 5.0,\n       \"sexual\": \"Medium\",\n       \"sexual_score\": 5.0,\n       \"\
  self_harm\": \"Medium\",\n       \"self_harm_score\": 5.0,\n       \"hate_unfairness\"\
  : \"Medium\",\n       \"hate_unfairness_score\": 5.0,\n   }\n   ````"
constructor:
  syntax: 'ContentSafetyChatEvaluator(azure_ai_project: dict, eval_last_turn: bool
    = False, parallel: bool = True, credential=None)'
  parameters:
  - name: azure_ai_project
    description: 'The scope of the Azure AI project.

      It contains subscription id, resource group, and project name.'
    isRequired: true
    types:
    - <xref:azure.ai.evaluation.AzureAIProject>
  - name: eval_last_turn
    description: 'Set to True to evaluate only the most recent exchange in the dialogue,

      focusing on the latest user inquiry and the assistant''s corresponding response.
      Defaults to False'
    defaultValue: 'False'
    types:
    - <xref:bool>
  - name: parallel
    description: 'If True, use parallel execution for evaluators. Else, use sequential
      execution.

      Default is True.'
    defaultValue: 'True'
    types:
    - <xref:bool>
  - name: credential
    description: The credential for connecting to Azure AI project.
    defaultValue: None
    types:
    - <xref:azure.core.credentials.TokenCredential>
